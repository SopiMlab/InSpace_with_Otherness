# InSpace with Otherness

InSpace with Otherness is an interactive music installation based on the NSynth neural audio synthesis algorithm. A generative background composition plays from the six speakers surrounding the installation space. A visitor can perform along with the composition by waving a magic staff around the space, triggering sounds generated by the NSynth depending on the position of the staff.

The installation is built using:

- [NSynth](https://magenta.tensorflow.org/nsynth)
- Kinect for Xbox 360
- [Processing](https://puredata.info/) + [Kinect4WinSDK](https://github.com/chungbwc/Kinect4WinSDK)
- [Pure Data](https://puredata.info/) + [pyext](https://github.com/SopiMlab/py)
- [SuperCollider](https://supercollider.github.io/)

## NSynth sample generation

A list of pitches is provided as input to NSynth, as well as four different instrument samples for each pitch. The samples are converted into a latent representation of timbre and dynamics, called *embeddings*, which allows for interpolation between the characteristics of different instruments. The embeddings for each instrument are placed in a corner of a 9×9 grid. For each in-between position on the grid, interpolated embeddings are generated and reconstructed into sound samples.

We prepared two sets of four instruments, using a phase vocoder based pitch shifter to generate pitched samples over two octaves, with four pitches (C, Eb, Gb, A) per octave.

Our first attempt did not produce coherently pitched output. These samples (continuous1, discrete1, lasi1, water) have quite complex timbres and lack a clear singular pitch, and NSynth perhaps has trouble analyzing such sounds.

For the second attempt we recorded more clearly pitched sounds:

- Glass struck with mallet
- Low piano string struck with mallet
- Whistling
- Wooden xylophone-like instrument struck with mallet

This produced much better output. Some of the resultant notes still seem incorrectly pitched, but this may be an unavoidable limitation of the NSynth algorithm. These sounds were incorporated into our composition.

Samples were generated using the [Triton](https://scicomp.aalto.fi/triton/) computing cluster provided by the Aalto University Science-IT project. We have a [guide](https://github.com/SopiMlab/open-nsynth-super/blob/master/audio/readme-triton.md) for running the generation scripts on Triton.

## NSynth playback

The output of the sample generation is a .bin file intended to be used with the [Open NSynth Super](https://github.com/SopiMlab/open-nsynth-super) device. We developed Pure Data objects to read these files and access the sample data, with the help of the pyext Pd-to-Python interface. Using this, we built a basic emulation of the Open NSynth Super with 4-note polyphony. To play pitches for which we didn't generate samples, we select the nearest generated sample and pitch it up or down to the target pitch.

## Object tracking

The object tracking system is built as a Processing sketch. We receive color and depth images from a Kinect positioned in front of the square area at the center of the installation. At the end of magic staff is an orb painted with a two-color pattern. Using color masking and blob detection, we detect blobs of both colors in the RGB image, and filter out those that do not appear close to at least two blobs of the other color. For the remaining blobs, we compute z position using the depth image, and finally average the x and z positions to get an approximate 2D position of the orb. This position is mapped to the NSynth's 9×9 grid of timbres. Whenever the orb moves to a different tile of the grid, we play the next note from a predetermined sequence in the NSynth timbre corresponding to that tile.

## Backing composition

The backing composition is written in SuperCollider and consists of five layers.

### Refrigerator pad

A recording of the hum from a grocery store refridgerator is used as the source for granular synthesis. Random 300ms grains are played at a rate of 30 per second and fed through a low-pass filter and an attack-decay envelope. Random notes from a Cm7/Bb chord are played on this synth, with duration and filter cutoff also randomized.

### Glitch

This layer is built from random patterns of four different short percussive sound generators:

- Single-sample impulses transformed by randomized FFT magnitude shifting and stretching
- FM blips with FFT bin scrambling
- Kick-drum-like sine wave hits with a short falling pitch envelope, also with FFT bin scrambling
- Chaotic noise generator with randomized FFT magnitude shifting and stretching, with a fading-in amplitude envelope

### Synth drone

First, a Cm chord played on a patch based on the "FACT Lead" preset for the Europa by Reason spectral wavetable synthesizer is rendered to audio. Two loops of the resulting sample are played on top of each other, one backwards and the other forwards, with different loop durations.

### Grains

Random grains between 0.4 and 1.5 seconds in duration are played from a non-tonal sample with some reverb added.

### Percussion

Recordings of knocking on wooden, metallic and plastic surfaces in our studio are played and processed with some distortion.